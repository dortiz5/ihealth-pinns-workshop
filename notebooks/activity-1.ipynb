{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks Vs. Physics-Informed Neural networks\n",
    "\n",
    "By David Ortiz and Rodrigo Salas, 2024\n",
    "\n",
    "Read the seminal PINNs paper [here](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "## Activity overview\n",
    "\n",
    "In this activity, we will code an Artificial Neural Network (ANN) and a Physics-Informed Neural Network (PINN) to solve the nonlinear mathematical model of an **oscillating pendulum**. This will allow us to explore the benefits of incorporating physical models into the loss function.\n",
    "\n",
    "## Activity goals\n",
    "\n",
    "By the end of this activity, you should be able to:\n",
    "\n",
    " - understand the need of numerical solution to complex models\n",
    " - understand the advantages of using Physics-Informed Neural Networks (PINNs) compared to common Artificial Neural Networks (ANN)\n",
    " - train data-driven PINNs using PyTorch\n",
    " - solve non-linear models using PINNs\n",
    "\n",
    "\n",
    "## Mathematical description of the problem\n",
    "We want to solve the mathematical problem related to the **oscillating pendulum**[(wiki)](https://en.wikipedia.org/wiki/Pendulum_(mechanics)):\n",
    "\n",
    "\n",
    "| ![GIF](../data/figures/Oscillating_pendulum.gif) | <img src=\"../data/figures/Pendulum_gravity.svg\" alt=\"Diagrama del proyecto\" width=\"300\"/> |\n",
    "|-------------------------------------------|-------------------------------------------|\n",
    "| Pendulum velocity and acceleration vectors  | Force diagram |\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "- the rod is rigid and massless [(Homework)](https://en.wikipedia.org/wiki/Elastic_pendulum#:~:text=In%20physics%20and%20mathematics%2C%20in,%2Ddimensional%20spring%2Dmass%20system.)\n",
    "- the weight is a point mass\n",
    "- two dimensions [(Homework)](https://www.instagram.com/reel/CffUr64PjCx/?igsh=MWlmM2FscG9oYnp6bw%3D%3D)\n",
    "- no air resistance [(Homework)](https://www.youtube.com/watch?v=erveOJD_qv4&ab_channel=Lettherebemath)\n",
    "- gravitational field is uniform and the support does not move\n",
    "\n",
    "We are interested in find the vertical angle $\\theta(t) \\in [0, 2\\pi)$ such that:\n",
    "\n",
    "$$\n",
    "\\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\sin\\theta=0,\\quad\\theta(0)=\\theta_0,\\quad\\theta'(0)=0,\\quad t\\in\\mathbb{R}, \n",
    "$$\n",
    "\n",
    "where $g\\approx9.81m/s^2$, $l$ is the length of the rod and $t$ the temporal variable.\n",
    "\n",
    "Review on differential equations:\n",
    "- Why is this a non-linear differential equation?\n",
    "- It is an ordinary differential equation (ODE) or a partial differential equation (PDE)?\n",
    "- Which is the order, Which is the degree?\n",
    "\n",
    "A usefull method is to convert the model to a coupled system of EDOs:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\theta}{dt} &= \\omega, \\quad \\text{angular velocity}\\\\\n",
    "\\frac{d\\omega}{dt} & = -\\frac{g}{l}\\sin\\theta, \\quad \\text{angular acceleration}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "The workflow for this activity will be the following:\n",
    "\n",
    "1. run a numerical solver. We will use the numerical solution as trining data\n",
    "2. train an ANN and show the solution. Play with training parameters to improve the outcome\n",
    "3. train a PINN and show the solution. Change training parameters and train a data-free model\n",
    "4. further discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 16,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Function to calculate the signal-to-noise ratio\n",
    "def calculate_snr(signal, noise):    \n",
    "    # Ensure numpy arrays\n",
    "    signal, noise = np.array(signal), np.array(noise)\n",
    "    \n",
    "    # Calculate the power of the signal and the noise\n",
    "    signal_power = np.mean(signal**2)\n",
    "    noise_power = np.mean(noise**2)\n",
    "    \n",
    "    # Calculate the SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Function to plot the solutions\n",
    "def plot_comparison(time, theta_true, theta_pred, loss):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    t_np = time.detach().numpy()\n",
    "    theta_pred_np = theta_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 2 subplots\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true and predicted values\n",
    "    axs[0].plot(t_np, theta_true, label = r'$\\theta(t)$ (numerical solution)')\n",
    "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
    "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
    "    axs[0].set_xlabel(r'Time $(s)$')\n",
    "    axs[0].set_ylabel('Amplitude') \n",
    "    axs[0].legend(loc='lower left', frameon=False)\n",
    "\n",
    "\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(theta_true.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
    "    axs[1].plot(t_np, difference)\n",
    "    axs[1].set_title('Absolute Difference')\n",
    "    axs[1].set_xlabel(r'Time $(s)$')\n",
    "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
    "    # Display the plot\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 1 subplots\n",
    "    _, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    axs.plot(loss)\n",
    "    axs.set_xlabel('Iteration')\n",
    "    axs.set_ylabel('Loss')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xscale('log')\n",
    "    axs.set_title('Training Progress')\n",
    "    axs.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical solution\n",
    "For the numerical solution we use the [Runge-Kutta of forth order](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) from `scipy`. We begin by defining the parameters for this example, the pendulum model, and the domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81  # gravity acceleration (m/s^2)\n",
    "L = 1.0   # Pendulum's rod length (m)\n",
    "theta0 = np.pi / 4  # Initial condition (Position in rads)\n",
    "omega0 = 0.0        # Initial angular speed (rad/s)\n",
    "\n",
    "# Simulation time (sample rate 100Hz)\n",
    "t_span = (0, 10)  # from 0 to 10 seconds\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 1000)  # Points to be evaluated\n",
    "\n",
    "# We define the system of coupled ODEs\n",
    "def pendulum(t, y):\n",
    "    theta, omega = y\n",
    "    dtheta_dt = omega\n",
    "    domega_dt = -(g / L) * np.sin(theta)\n",
    "    return [dtheta_dt, domega_dt]\n",
    "\n",
    "# Initial conditions\n",
    "y0 = [theta0, omega0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we solve the problem numerically using `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Solve the initial value problem using Runge-Kutta 4th order\n",
    "sol = solve_ivp(pendulum, t_span, y0, t_eval=t_eval, method='RK45')\n",
    "\n",
    "# We extract the solutions\n",
    "theta = sol.y[0]\n",
    "omega = sol.y[1]\n",
    "\n",
    "# We graph the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta, label=r'$\\theta(t)$ (Angular Displacement)')\n",
    "plt.plot(t_eval, omega, label=r'$\\omega(t)$ (Angular Velocity)')\n",
    "plt.xlabel(r'Time $(s)$')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Nonlinear Pendulum Solution')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Artificial Neural Network\n",
    "\n",
    "In the following, we consider the numerical solution as the **training data**. We add gaussian noise, subsample and cut the data to $2.5s$ to test the performance of the ANN. Also, we calculate the signal-to-noise ratio $SNR = 10\\log_{10} \\left(\\frac{P_{signal}}{P_{noise}}\\right)$, where $P_{signal}$ and $P_{noise}$ are the power of the signal and the noise, respectively, to get the amount of distortion in the signal. We call the noisy training data $\\theta_{data}(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gaussian noise\n",
    "sigma = 0.05\n",
    "noise = np.random.normal(0,sigma,theta.shape[0])\n",
    "theta_noisy = theta + noise\n",
    "print(f'SNR: {calculate_snr(theta_noisy, noise):.4f} dB')\n",
    "\n",
    "# Resample and cut to 2.5s\n",
    "resample = 5          # resample \n",
    "ctime = int(2.5*100)  # 2.5s times 100Hz\n",
    "\n",
    "theta_data = theta_noisy[:ctime:resample]\n",
    "t_data = t_eval[:ctime:resample]\n",
    "\n",
    "# We graph the observed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta, label=r'$\\theta(t)$ (Angular Displacement)')\n",
    "plt.plot(t_data, theta_data, label=r'$\\theta_{data}(t)$ (Training data)')\n",
    "plt.xlabel(r'Time $(s)$')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.title('Training data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the artificial neural network to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{NN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [1, 50, 50, 50, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_phys = torch.tensor(t_eval, requires_grad=True).float().reshape(-1,1)\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, requires_grad=True).float().reshape(-1,1)\n",
    "# Numerical theta to test Numpy array to pytorch tensor \n",
    "theta_test = torch.tensor(theta, requires_grad=True).float().reshape(-1,1)\n",
    "# Numerical theta to train Numpy array to pytorch tensor \n",
    "theta_data = torch.tensor(theta_data, requires_grad=True).float().reshape(-1,1)\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network \n",
    "theta_nn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in theta_nn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_nn.parameters(), lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "To train the ANN, it is mandatory to define the loss function. To this end, we consider the noisy data $\\theta_{data}(t)$ and use the mean squared error ($MSE$) over the colocation points (samples over the domain) $\\{t_i\\}_N$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) := \\lambda_1 MSE(\\theta_{NN}(t; \\Theta), \\theta_{data}(t)) = \\frac{\\lambda_1}{N}\\sum_i (\\theta_{NN}(t_i; \\Theta) - \\theta_{data}(t_i))^2\n",
    "$$\n",
    "\n",
    "where $\\lambda_1\\in\\mathbb{R}^+$ is a positive (weigth) number, and $N$ is the number of samples. The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we use `PyTorch` and define the neural network, the function loss and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetworkLoss(forward_pass, t, theta_data, lambda1 = 1):\n",
    "    \n",
    "    theta_nn = forward_pass(t)\n",
    "    data_loss = lambda1 * MSE_func(theta_nn, theta_data)\n",
    "    \n",
    "    return  data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = NeuralNetworkLoss(theta_nn,\n",
    "                             t_data,\n",
    "                             theta_data)    # must be (1. nn output, 2. target)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred = theta_nn(t_phys)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred, theta_test)}')\n",
    "\n",
    "plot_comparison(t_phys, theta, theta_pred, loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. try changing the `sigma`, the `resample` and the `ctime` variables to lower or higher values and test the performance of the ANN\n",
    "2. increase and decrease the `lambda1` parameters of the loss function\n",
    "3. increase and reduce the learning rate of the optimizer\n",
    "4. change the architecture of the ANN\n",
    "5. increase the number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Physics-Informed Neural Network\n",
    "For this task we use the same noisy **training data** but in this case, we train the PINN to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{PINN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network\n",
    "theta_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in theta_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_pinn.parameters(), lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the pendulum model and define function $f_{ode}(t;g,l)$, $g_{ic}(0)$ and $h_{bc}(0)$ for the ODE, the initial condition and the boundary condition. Also, we replace the analytical solution $\\theta(t)$ with the PINN output $\\theta_{pinn}(t; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{ode}(t;\\theta_{pinn}):=&\\frac{d^2\\theta_{PINN}(t; \\Theta)}{dt^2}+\\frac{g}{l}\\sin(\\theta_{pinn}(t; \\Theta)) = 0\\\\\n",
    "g_{ic}(0;\\theta_{pinn}):=&\\theta_{pinn}(0; \\Theta) = \\theta_0\\\\\n",
    "h_{bc}(0;\\theta_{pinn}):=&\\theta_{pinn}'(0; \\Theta) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):= &\\frac{\\lambda_1}{N}\\sum_i\\left(f_{ode}(t_i;\\theta_{pinn})-0\\right)^2 \\quad \\text{ODE loss}\\\\\n",
    "                   & + \\lambda_2 (g_{ic}(0;\\theta_{pinn})-\\theta_0)^2 \\quad \\text{IC loss}\\\\\n",
    "                   & + \\lambda_3 (h_{bc}(0;\\theta_{pinn})-0)^2 \\quad \\text{BC loss}\\\\\n",
    "                   & + \\frac{\\lambda_4}{N}\\sum_i (\\theta_{pinn}(t_i; \\Theta) - \\theta_{data}(t_i))^2 \\quad \\text{DATA loss}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2,3,4}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> when we do not include the loss function related to the data, we are employing a data-free scheme; when we include the data, we are employing a data-driven scheme.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Autodifferentiation (torch.autograd) is a powerful tool for calculating the gradients of the PINN with respect to its input to evaluate the loss function; for more information, refer to the tutorial.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t = 0 for boundary an initial conditions \n",
    "t0 = torch.tensor(0., requires_grad=True).view(-1,1)\n",
    "\n",
    "# HINT: use grad funtion (a wraper for torch.autograd) to calculate the \n",
    "# derivatives of the ANN\n",
    "def PINNLoss(forward_pass, t_phys, t_data, theta_data, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    theta_pinn1 = forward_pass(t_phys)\n",
    "    #TODO: calculate the first and second derivatives\n",
    "    \n",
    "    #TODO: calculate the ODE loss\n",
    "    \n",
    "    g_ic = forward_pass(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*theta0)\n",
    "    \n",
    "    #TODO: calculate boundary condition\n",
    "    \n",
    "    theta_nn2 = forward_pass(t_data)\n",
    "    data_loss = lambda4 * MSE_func(theta_nn2, theta_data)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss + data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(theta_pinn, t_phys, t_data, theta_data)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred = theta_pinn(t_phys)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred, theta_test)}')\n",
    "\n",
    "plot_comparison(t_phys, theta, theta_pred, loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. try changing the `sigma`, the `resample` and the `ctime` variables to lower or higher values and test the performance of the ANN\n",
    "2. increase and decrease the `lambdas` parameters of the loss function\n",
    "3. increase and reduce the learning rate of the optimizer\n",
    "4. change the architecture of the ANN\n",
    "5. increase the number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
