{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse applications\n",
    "\n",
    "By David Ortiz and Rodrigo Salas, 2024\n",
    "\n",
    "Interesting paper about [parameter estimation](https://arxiv.org/abs/2308.00927) in blood flown problem.\n",
    "\n",
    "## Activity overview\n",
    "\n",
    "In this activity, we will code a PINN to estimate the diffusion coefficient of the linear 1D heat equation.\n",
    "\n",
    "## Activity goals\n",
    "\n",
    "By the end of this activity, you should be able to:\n",
    "\n",
    " - use the PINN method to solve inverse problem,\n",
    " - while, training a PINN to solve the diffusion problem\n",
    "\n",
    "\n",
    "## Mathematical description of the problem\n",
    "In this activity, we will consider the one dimensional diffusion model, usually called 1D heat equation, defined on activity 2, i.e., \n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\text{PDE:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 1]. \\quad \\kappa\\in\\mathbb{R}\\\\\n",
    "    \\text{Solution:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where $u(t,x)$ is the quantity of interest (e.g., temperature, concentration) at the point $x\\in[-1,1]$ and at time $t\\in[0,1]$ and $\\kappa = 1$ **is the diffusion coefficient that we want to estimate**. The main idea is to treat $\\kappa$ as a free parameter when the PINN is train. Thus, we train the PINN to solve the model, while we find the true value of the parameter. \n",
    "\n",
    "Notice that for this case, we do not need the initial conditions and the boundary conditions, but we need additional information in terms of noisy observations $u_{data}(t,x)$. This will be explained below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 16,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# torch definition of pi number\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "\n",
    "\n",
    "# Function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "\n",
    "# Function to plot the solutions\n",
    "def plot_comparison(u_true, u_pred, loss, k_evol):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    u_pred_np = u_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 4 subplots\n",
    "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values\n",
    "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
    "    axs[0].set_title('Analytic solution for diffusion')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im1, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "\n",
    "    # Plot the predicted values\n",
    "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
    "    axs[1].set_title('PINN solution for diffusion')\n",
    "    axs[1].set_xlabel(r'$x$')\n",
    "    axs[1].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im2, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[1])\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 2 subplots\n",
    "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Plot the difference between the predicted and true values\n",
    "    axs[0].plot(k_evol, label=\"PINN estimate\")\n",
    "    axs[0].hlines(1, 0, len(k_evol), label=\"True value\", color=\"tab:green\")\n",
    "    axs[0].set_title(r\"$\\kappa$ evolution\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    \n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_title('Training Progress')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,  \n",
    "                        )[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution (observations)\n",
    "Once again, we define the analytical solution $u(t,x) = e^{-t}\\sin(\\pi x)$. For this task, the `analytic_diffusion` will be used to generate the observation data and as reference for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in x and t\n",
    "dom_samples = 100\n",
    "\n",
    "# Function for the diffusion analytical solution\n",
    "def analytic_diffusion(x,t):\n",
    "    u = np.exp(-t)*np.sin(np.pi*x)\n",
    "    return u\n",
    "\n",
    "# spatial domain\n",
    "x = np.linspace(-1, 1, dom_samples)\n",
    "# temporal domain\n",
    "t = np.linspace(0, 1, dom_samples)\n",
    "\n",
    "# Domain mesh\n",
    "X, T = np.meshgrid(x, t)\n",
    "U = analytic_diffusion(X, T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(U, extent=[-1,1,1,0])\n",
    "ax.set_title('Analytic solution for diffusion')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left', frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Physics-informed Neural Network\n",
    "To train the PINN, we will generate the domain using the `LatinHypercube` sampling (LHS) strategy. LHS ensures that the samples evenly cover the input space ensuring that the samples are not clustered in a small area, but are distributed over the entire space. \n",
    "\n",
    "We import the `qmc.LatinHypercube` from `scipy.stats` and scale it to the boundaries of the domain. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "# LHS sampling strategy\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(n=100)\n",
    "\n",
    "# lower and upper boundas of the domain\n",
    "l_bounds = [-1, 0]\n",
    "u_bounds = [ 1, 1]\n",
    "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# torch tensors\n",
    "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
    "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
    "ax.set_title('Collocation points')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we evaluate the `analytic_diffusion` function on this collocation points and add some noise to get the observation data $u_{data}(t,x)$, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate sample points in analytical function\n",
    "x_np = x_ten.detach().numpy()\n",
    "t_np = t_ten.detach().numpy()\n",
    "u_true = analytic_diffusion(x_np,t_np).reshape(1, -1)\n",
    "u_observ = u_true + np.random.normal(0,0.01,len(x_np))\n",
    "# convert observations in Pytorch tensors\n",
    "u_observ_t = torch.tensor(u_observ, requires_grad = True).float().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we will perform the $\\kappa$ parameter estimation at the same time as the PINN is trained. To this end, we train the ANN to directly approximate the solution to the partial differential equation and include $\\kappa$ as a free parameters, i.e.,\n",
    "\n",
    "$$\n",
    "u_{PINN}(t, x; \\Theta, \\kappa) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer.\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Parameter k is added to the optimizer, not to the ANN\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [2, 20, 20, 20, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network \n",
    "u_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "\n",
    "# treat k as a learnable parameter\n",
    "kappa = torch.nn.Parameter(torch.ones(1, requires_grad=True)*2)\n",
    "kappas = []\n",
    "\n",
    "# add k to the optimiser\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(list(u_pinn.parameters())+[kappa], lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the diffusion model and define function $f_{pde}(t, x)$ for the PDE. Also, we replace the analytical solution $u(t,x)$ with the PINN output $u_{pinn}(t,x; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u}{\\partial t} - \\kappa\\frac{\\partial^2 y}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2  \\sin(\\pi x)) = 0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> We do not need the boundary and initial conditions\n",
    "</div>\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i,x_i;u_{pinn})-0\\right)^2 \\quad \\text{PDE loss}\\\\\n",
    "                   & + \\frac{\\lambda_2}{N}\\sum_i\\left(u_{PINN}(t_i,x_i; \\Theta) - \\theta_{data}(t_i,x_i)\\right)^2 \\quad \\text{DATA loss}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> For inverse problem data-driven escheme is used.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten,\n",
    "             lambda1 = 1, lambda2 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
    "    u = forward_pass(domain)\n",
    "    u_t = grad(u, t_ten)\n",
    "    u_x = grad(u, x_ten)\n",
    "    u_xx = grad(u_x, x_ten)\n",
    "    \n",
    "    # PDE loss definition\n",
    "    f_pde = u_t - kappa*u_xx + torch.exp(-t_ten)*(torch.sin(np.pi*x_ten) \n",
    "                                        -(torch.pi**2)*torch.sin(np.pi*x_ten))\n",
    "    PDE_loss = lambda1 * MSE_func(f_pde, torch.zeros_like(f_pde)) \n",
    "    \n",
    "    # Data loss\n",
    "    data_loss = lambda2 * MSE_func(u, u_observ_t)\n",
    "    \n",
    "    return PDE_loss + data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    kappas.append(kappa.item())\n",
    "\n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
    "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
    "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
    "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
    "\n",
    "U_true = torch.tensor(U).float()\n",
    "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
    "\n",
    "plot_comparison(U, U_pred, loss_values, kappas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Increase the initial guess for the parameter. Consider that en many applications we only have a searching interval for some parameters.\n",
    "2. increase and decrease the `lambdas` parameters of the loss function\n",
    "3. increase and reduce the learning rate of the optimizer\n",
    "4. change the architecture of the ANN\n",
    "5. increase the number of training iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse problem on pendulum\n",
    "\n",
    "We are now experts on PINNs and inverse problems! A nice exercise will be to estimate the gravity and the length for the pendulum in activity 1.\n",
    "\n",
    "$$\n",
    "\\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\sin\\theta=0,\\quad\\theta(0)=\\theta_0,\\quad\\theta'(0)=0,\\quad t\\in\\mathbb{R}, \n",
    "$$\n",
    "\n",
    "where $g\\approx9.81m/s^2$, $l = 1 m$ is the length of the rod and $t$ the temporal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Function to calculate the signal-to-noise ratio\n",
    "def calculate_snr(signal, noise):    \n",
    "    # Ensure numpy arrays\n",
    "    signal, noise = np.array(signal), np.array(noise)\n",
    "    \n",
    "    # Calculate the power of the signal and the noise\n",
    "    signal_power = np.mean(signal**2)\n",
    "    noise_power = np.mean(noise**2)\n",
    "    \n",
    "    # Calculate the SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "\n",
    "# Function to plot the solutions\n",
    "def plot_comparison(time, theta_true, theta_pred, loss, gravs, lengths):\n",
    "    \n",
    "\n",
    "    # Create a figure with 4 subplots\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    t_np = time.detach().numpy()\n",
    "    theta_pred_np = theta_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 2 subplots\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true and predicted values\n",
    "    axs[0].plot(t_np, theta_true, label = r'$\\theta(t)$ (numerical solution)')\n",
    "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
    "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
    "    axs[0].set_xlabel(r'Time $(s)$')\n",
    "    axs[0].set_ylabel('Amplitude') \n",
    "    axs[0].legend(loc='lower left', frameon=False)\n",
    "\n",
    "\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(theta_true.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
    "    axs[1].plot(t_np, difference)\n",
    "    axs[1].set_title('Absolute Difference')\n",
    "    axs[1].set_xlabel(r'Time $(s)$')\n",
    "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
    "    # Display the plot\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 2 subplots\n",
    "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Plot the difference between the predicted and true values\n",
    "    axs[0].plot(gravs, label=\"PINN estimate\")\n",
    "    axs[0].hlines(9.81, 0, len(gravs), label=\"True value\", color=\"tab:red\")\n",
    "    axs[0].plot(lengths, label=\"PINN estimate\")\n",
    "    axs[0].hlines(1, 0, len(lengths), label=\"True value\", color=\"tab:red\")\n",
    "    axs[0].set_title(r\"$\\kappa$ evolution\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    \n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_title('Training Progress')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = 9.81  # gravity acceleration (m/s^2)\n",
    "L = 1.0   # Pendulum's rod length (m)\n",
    "theta0 = np.pi / 4  # Initial condition (Position in rads)\n",
    "omega0 = 0.0        # Initial angular speed (rad/s)\n",
    "\n",
    "# Simulation time (sample rate 100Hz)\n",
    "t_span = (0, 10)  # from 0 to 10 seconds\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 1000)  # Points to be evaluated\n",
    "\n",
    "# We define the system of coupled ODEs\n",
    "def pendulum(t, y):\n",
    "    theta, omega = y\n",
    "    dtheta_dt = omega\n",
    "    domega_dt = -(g / L) * np.sin(theta)\n",
    "    return [dtheta_dt, domega_dt]\n",
    "\n",
    "# Initial conditions\n",
    "y0 = [theta0, omega0]\n",
    "\n",
    "# Solve the initial value problem using Runge-Kutta 4th order\n",
    "sol = solve_ivp(pendulum, t_span, y0, t_eval=t_eval, method='RK45')\n",
    "\n",
    "# We extract the solutions\n",
    "theta = sol.y[0]\n",
    "omega = sol.y[1]\n",
    "\n",
    "# We graph the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta, label=r'$\\theta(t)$ (Angular Displacement)')\n",
    "plt.plot(t_eval, omega, label=r'$\\omega(t)$ (Angular Velocity)')\n",
    "plt.xlabel(r'Time $(s)$')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Nonlinear Pendulum Solution')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gaussian noise\n",
    "#TODO: change sigma\n",
    "sigma = 0.05\n",
    "noise = np.random.normal(0,sigma,theta.shape[0])\n",
    "theta_noisy = theta + noise\n",
    "print(f'SNR: {calculate_snr(theta_noisy, noise):.4f} dB')\n",
    "\n",
    "# Resample to 5Hz and cut to 2.5s\n",
    "resample = 4         # resample to 5Hz \n",
    "ctime = int(10*100)  # 2.5s times 100Hz\n",
    "\n",
    "theta_data = theta_noisy[:ctime:resample]\n",
    "t_data = t_eval[:ctime:resample]\n",
    "\n",
    "# We graph the observed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta, label=r'$\\theta(t)$ (Angular Displacement)')\n",
    "plt.plot(t_data, theta_data, label=r'$\\theta_{data}(t)$ (Training data)')\n",
    "plt.xlabel(r'Time $(s)$')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.title('Training data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Informed neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_phys = torch.tensor(t_eval, requires_grad=True).float().reshape(-1,1)\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, requires_grad=True).float().reshape(-1,1)\n",
    "# Numerical theta to test Numpy array to pytorch tensor \n",
    "theta_test = torch.tensor(theta, requires_grad=True).float().reshape(-1,1)\n",
    "# Numerical theta to train Numpy array to pytorch tensor \n",
    "theta_data = torch.tensor(theta_data, requires_grad=True).float().reshape(-1,1)\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [1, 50, 50, 50, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the PINN and add the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create the PINN\n",
    "\n",
    "\n",
    "#TODO: create parameters to estimate\n",
    "\n",
    "\n",
    "#TODO: create optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we train the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t = 0 for boundary an initial conditions \n",
    "t0 = torch.tensor(0., requires_grad=True).view(-1,1)\n",
    "\n",
    "# HINT: \n",
    "def PINNLoss(forward_pass, t_phys, t_data, theta_data, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1):\n",
    "\n",
    "    #TODO: define the PINN loss\n",
    "    \n",
    "    return #ODE_loss + IC_loss + BC_loss + data_loss\n",
    "\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    #TODO: initialize gradients\n",
    "    \n",
    "    #TODO: evaluate PINNloss\n",
    "    \n",
    "    #TODO: save values\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    \n",
    "    #TODO: perform backpropagation\n",
    "    \n",
    "    \n",
    "    #TODO: update waeigths\n",
    "    \n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
