{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward applications\n",
    "\n",
    "By David Ortiz and Rodrigo Salas, 2024\n",
    "\n",
    "## Activity overview\n",
    "\n",
    "In this activity, we will code a PINN for the solution of a linear diffusion model. \n",
    "\n",
    "## Activity goals\n",
    "\n",
    "By the end of this activity, you should be able to:\n",
    "\n",
    " - define test PDE problems with custom analytical solutions\n",
    " - train PINN to solve PDE models\n",
    "\n",
    "\n",
    "## Mathematical description of the problem\n",
    "In this activity, we will consider the one dimensional diffusion model, usually called 1D heat equation,\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} + f(t,x), \\quad && x \\in [-1, 1], \\quad t \\in [0, 1]. \\quad \\kappa\\in\\mathbb{R} \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where $u(t,x)$ is the quantity of interest (e.g., temperature, concentration) at the point $x\\in[-1,1]$ and at time $t\\in[0,1]$, $\\kappa$ is the diffusion coefficient, and $f(t,x)$ is a source term.\n",
    "\n",
    "Here, we will fabricate our oun model. Thus, we propose as analytic function\n",
    "$$\n",
    "u(t,x) = e^{-t}\\sin(\\pi x)\n",
    "$$\n",
    "\n",
    "By deriving and replacing in the PDE, we obtain the following problem with initial and boundary conditions\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\text{PDE:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 1]. \\quad \\kappa\\in\\mathbb{R} \\\\\n",
    "    \\text{IC:} \\quad & u(0,x) &&= \\sin(\\pi x), && x \\in [-1, 1] \\\\\n",
    "    \\text{BC:} \\quad & u(t,-1)&& = u(t,1)= 0, && t \\in [0, 1] \\\\\n",
    "    \\text{Solution:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Without loss of generality, we will assume $\\kappa = 1$ for this activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 16,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# torch definition of pi number\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "\n",
    "\n",
    "# Function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "\n",
    "# Function to plot the solutions\n",
    "def plot_comparison(u_true, u_pred, loss):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    u_pred_np = u_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 4 subplots\n",
    "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values\n",
    "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
    "    axs[0].set_title('Analytic solution for diffusion')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im1, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "\n",
    "    # Plot the predicted values\n",
    "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
    "    axs[1].set_title('PINN solution for diffusion')\n",
    "    axs[1].set_xlabel(r'$x$')\n",
    "    axs[1].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im2, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[1])\n",
    "        # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with  subplots\n",
    "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(u_true - u_pred_np)\n",
    "    im3 = axs[0].imshow(difference, extent=[-1,1,1,0])\n",
    "    axs[0].set_title(r'$|u(t,x) - u_{pred}(t,x)|$')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$') \n",
    "    fig2.colorbar(im3, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "    \n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_title('Training Progress')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,  \n",
    "                        )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution\n",
    "We define the analytical solution $u(t,x) = e^{-t}\\sin(\\pi x)$ for the diffusion problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in x and t\n",
    "dom_samples = 100\n",
    "\n",
    "# Function for the diffusion analytical solution\n",
    "def analytic_diffusion(x,t):\n",
    "    u = np.exp(-t)*np.sin(np.pi*x)\n",
    "    return u\n",
    "\n",
    "# spatial domain\n",
    "x = np.linspace(-1, 1, dom_samples)\n",
    "# temporal domain\n",
    "t = np.linspace(0, 1, dom_samples)\n",
    "\n",
    "# Domain mesh\n",
    "X, T = np.meshgrid(x, t)\n",
    "U = analytic_diffusion(X, T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(U, extent=[-1,1,1,0])\n",
    "ax.set_title('Analytic solution for diffusion')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left', frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Physics-informed Neural Network\n",
    "To train the PINN, we will generate the domain using the `LatinHypercube` sampling (LHS) strategy. LHS ensures that the samples evenly cover the input space ensuring that the samples are not clustered in a small area, but are distributed over the entire space. \n",
    "\n",
    "We import the `qmc.LatinHypercube` from `scipy.stats` and scale it to the boundaries of the domain. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "# LHS sampling strategy\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(n=100)\n",
    "\n",
    "# lower and upper boundas of the domain\n",
    "l_bounds = [-1, 0]\n",
    "u_bounds = [ 1, 1]\n",
    "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# torch tensors\n",
    "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
    "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
    "ax.scatter(domain_xt[:, 0],np.zeros_like(domain_xt[:, 1]), label = 'IC collocation points')\n",
    "ax.scatter(np.ones_like(domain_xt[:, 0]),domain_xt[:, 1], label = 'BC1 collocation points')\n",
    "ax.scatter(np.ones_like(domain_xt[:, 1])*-1,domain_xt[:, 1], label = 'BC2 collocation points')\n",
    "ax.set_title('Collocation points')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the artificial neural network to directly approximate the solution to the partial differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "u_{PINN}(t, x; \\Theta) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [2, 10, 10, 10, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network \n",
    "u_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(u_pinn.parameters(), lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the diffusion model and define function $f_{pde}(t, x)$, $g_{ic}(0)$ and $h_{bc}(0)$ for the PDE, the initial condition and the boundary condition. Also, we replace the analytical solution $u(t,x)$ with the PINN output $u_{pinn}(t,x; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u}{\\partial t} - \\frac{\\partial^2 y}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)) = 0\\\\\n",
    "g_{ic}(0,x;u_{pinn}):=&u_{pinn}(0,x; \\Theta) = \\sin(\\pi x)\\\\\n",
    "h_{bc1}(t,-1;u_{pinn}):=&u_{pinn}(t,-1; \\Theta) = 0\\\\\n",
    "h_{bc2}(t,1;u_{pinn}):=&u_{pinn}(t,1; \\Theta) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i, x_i;u_{pinn})-0\\right)^2 \\quad \\text{PDE loss}\\\\\n",
    "                   & + \\frac{\\lambda_2}{N} \\sum_i(g_{ic}(0,x_i;u_{pinn})-\\sin(\\pi x_i))^2 \\quad \\text{IC loss}\\\\\n",
    "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc1}(t_i,-1;u_{pinn})-0)^2 \\quad \\text{BC1 loss}\\\\\n",
    "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc2}(t_i,1;u_{pinn})-0)^2 \\quad \\text{BC2 loss}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2,3}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> when we do not include the loss function related to the data, we are employing a data-free scheme; when we include the data, we are employing a data-driven scheme.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Autodifferentiation (torch.autograd) is a powerful tool for calculating the gradients of the PINN with respect to its input to evaluate the loss function; for more information, refer to the tutorial.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: \n",
    "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
    "    u = forward_pass(domain)\n",
    "    #TODO:  Calculate the first and second derivatives\n",
    "    \n",
    "    # PDE loss definition\n",
    "    #TODO: calculate the PDE loss\n",
    "    \n",
    "    # IC loss definition\n",
    "    #TODO: calculate the IC loss\n",
    "\n",
    "    # BC x = -1 definition\n",
    "    #TODO: calculate the BC1 loss\n",
    "    \n",
    "    # BC x = 1 definition\n",
    "    #TODO: calculate the BC2 loss\n",
    "\n",
    "    \n",
    "    return PDE_loss + IC_loss + BC1_loss + BC2_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
    "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
    "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
    "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
    "\n",
    "U_true = torch.tensor(U).float()\n",
    "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
    "\n",
    "plot_comparison(U, U_pred, loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Add the data driven-case. **HINT**: you must use the same collocation points for the analytic function and the PINN\n",
    "2. increase and decrease the `lambdas` parameters of the loss function\n",
    "3. increase and reduce the learning rate of the optimizer\n",
    "4. change the architecture of the ANN\n",
    "5. increase the number of training iterations\n",
    "6. discuss about convergence rate and the relative error. What do you notice compared to the non-linear exmple in activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
